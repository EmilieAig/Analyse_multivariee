---
title: "Analyse multivariée"
author: "AIGOIN Emilie"
date: "2025-10-05"
output: html_document
---

# Mise en place des données

### Chargement des packages nécessaires

```{r, warning = FALSE, message = FALSE}

library(dplyr)
library(pls)
library(glmnet)

```

### Ouvrir le fichier contenant le jeu de données

```{r, warning=FALSE}

# Ouvrir le fichier en précisant les séparateurs
data <- read.csv("Datagenus.csv", sep = ";", dec = ",", header = TRUE)

# Transform all character columns into numerical values
data <- data %>% mutate_if(is.character,~ as.numeric(.))

# Enlever la colonne forest
data <- data %>% select(-forest)
                          
```

### Mettre en place les sous jeux de données qui vont nous servir pour notre analyse

```{r}

# Données qui contiennent uniquement les variables d'espèces
data_gen <- data %>% select(num_range("gen", 1:27))

```

# Question 1.

Nous allons sommer les abondances des 27 espèces et diviser cette somme par la surface de chaque parcelle. On obtient ainsi la densité de peuplement arboré sur la parcelle.

```{r}

# Calcul de la densité
treedensity <- rowSums(data_gen) / data$surface

```

Nous continuons en calculant les carrés des variables géographiques quantitatives que sont la latitude, la longitude, l'altitude, la pluviométrie annuelle et les pluviométreis mensuelles.

```{r}

# Calculer les carrés des variables géographiques
var_geo_quanti <- c("lat", "lon", "altitude", "pluvio_yr", paste0("pluvio_", 1:12))
var_geo_carre <- data[, var_geo_quanti]^2
colnames(var_geo_carre) <- paste0(var_geo_quanti, "^2")

```

Puis nous formons les indicatrices de la variable geology. 

```{r}

# Créer les indicatrices de geology
data$geology <- as.factor(data$geology)
geo_indic <- model.matrix(~ geology - 1, data = data)

```

On voit qu'il n'y a pas de type 4 dans la colonne géologie.

```{r}

# Regrouper toutes les variables explicatives
var_explicatives <- bind_cols(
  data %>% select(lat, lon, altitude, pluvio_yr, num_range("pluvio_", 1:12)),  # Variables géographiques
  var_geo_carre,                                                               # Leurs carrés
  as.data.frame(geo_indic),                                                    # Indicatrices geology
  data %>% select(num_range("evi_", 1:23))                                     # EVI
)

```

Pour finir cette question, nous allons faire l'inventaire théorique de toutes les multi-colinéarités (combinaisons linéaires de variables qui s'annulent) présentées par l'ensemble des variables explicatives.

L'ensemble des variables explicatives présente deux multi-colinéarités certaines :

- Variables indicatrices de la géologie : les six indicatrices créées à partir de la variable geology vérifient la relation : $$ \text{geology}_1 + \text{geology}_2 + \text{geology}_3 + \text{geology}_4 + \text{geology}_5 + \text{geology}_6 = 1 $$ Cette égalité traduit le fait que chaque observation appartient à exactement une catégorie géologique. Donc, si on connaît 5/6 des variables, on peut forcément connaître les valeurs de la dernière.
- Pluviométrie annuelle et mensuelles : la pluviométrie annuelle `pluvio_yr` correspond à la somme des pluviométries mensuelles : $$ \text{pluvio_year} = \sum_{i=1}^{12} \text{pluvio}_i$$
Cette relation induit une dépendance linéaire entre ces treize variables.

Donc, l'ensemble des variables explicatives comporte deux combinaisons linéaires qui s'annulent, réduisant de deux unités le rang de la matrice des variables explicatives.

# Question 2.

*Réaliser une ACP réduite globale des variables explicatives. Retenir les composantes qui ne sont pas du bruit.*

```{r}

# ACP réduite (variables centrées et réduites)
acp <- prcomp(var_explicatives, scale. = TRUE, center = TRUE)

# Résumé de l'ACP
summary(acp)

# Variance expliquée par chaque composante
variance_expliquee <- acp$sdev^2 / sum(acp$sdev^2) * 100

# Afficher les variances expliquées
barplot(variance_expliquee[1:20], 
        names.arg = 1:20,
        xlab = "Composante principale", 
        ylab = "% de variance expliquée",
        main = "Variance expliquée par les composantes principales")

# Variance cumulée
variance_cumulee <- cumsum(variance_expliquee)

# Créer un tableau récapitulatif
recap_acp <- data.frame(
  Composante = 1:ncol(var_explicatives),
  Variance = acp$sdev^2,
  Variance_pct = variance_expliquee,
  Variance_cum = variance_cumulee
)

# Afficher les premières composantes
head(recap_acp, 20)

```

On choisit 3 composantes.

*Modéliser la densité à partir des composantes retenues. Vous pourrez éliminer les composantes qui n'ont pas de rôle statistiquement significatif (justifiez la validité des tests de Student ici). Donner le $R^2$ du modèle obtenu.*

```{r}

# Récupérer les 3 premières composantes principales
composantes_retenues <- acp$x[, 1:5]

```

```{r}

# Créer un data frame avec Y et les composantes
data_regression <- data.frame(
  treedensity = treedensity,
  PC1 = composantes_retenues[, 1],
  PC2 = composantes_retenues[, 2],
  PC3 = composantes_retenues[, 3]
)

```

```{r}

# Ajuster le modèle linéaire avec les 3 composantes
modele_complet <- lm(treedensity ~ PC1 + PC2 + PC3, data = data_regression)

# Afficher le résumé du modèle
summary(modele_complet)

```

*Construire le graphe d'abscisse $Y$ et d'ordonnée $\hat Y$, et commenter*

```{r}

Y <- treedensity
Y_chapeau <- fitted(modele_complet)

# Créer le graphique
plot(Y, Y_chapeau, 
     xlab = "Densité observée (Y)", 
     ylab = "Densité prédite (Ŷ)",
     main = "Valeurs observées vs prédites (R² = 0.204)",
     pch = 20,
     col = rgb(0, 0, 1, 0.5))

# Ajouter la droite y = x (droite identité)
abline(a = 0, b = 1, col = "red", lwd = 2)

# Ajouter une droite de régression pour voir l'ajustement
abline(lm(Y_chapeau ~ Y), col = "blue", lty = 2, lwd = 2)

legend("topright", 
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "blue"), 
       lty = c(1, 2), 
       lwd = 2,
       cex = 0.8)

# Calculer quelques statistiques pour le commentaire
correlation_YYchap <- cor(Y, Y_chapeau)
cat("Corrélation entre Y et Ŷ :", round(correlation_YYchap, 3), "\n")

```

*Retrouver les coefficients des variables originelles dans le prédicteur linéaire $\hat Y$.*

```{r}

# Coefficients des composantes dans le modèle (sans l'intercept)
coef_composantes <- coef(modele_complet)[-1]
print(coef_composantes)

```

```{r}

# Matrice de rotation de l'ACP pour les 3 premières composantes

rotation_matrix <- acp$rotation[, 1:3]

```

Formule : β_originels = Rotation × β_composantes
Car Ŷ = β₁·PC1 + β₂·PC2 + β₃·PC3
Et PCj = Σ(rotation[i,j] × X_i) donc on remplace

```{r}

# Calculer les coefficients des variables originelles
coef_originels <- rotation_matrix %*% coef_composantes

# Créer un data frame pour une meilleure visualisation
coef_df <- data.frame(
  Variable = rownames(coef_originels),
  Coefficient = as.vector(coef_originels)
)

# Trier par valeur absolue décroissante pour voir les plus importantes
coef_df <- coef_df[order(abs(coef_df$Coefficient), decreasing = TRUE), ]

# Afficher tous les coefficients
print(coef_df, row.names = FALSE)

```

*Si besoin est, corriger la linéarité de la liaison en utilisant une transformation de type Log sur $Y$ qui laisse les zéros invariants.*

```{r}

# Vérifier s'il y a des zéros ou valeurs négatives dans treedensity
cat("Valeur minimale :", min(treedensity), "\n")
cat("Valeur maximale :", max(treedensity), "\n\n")

# Appliquer la transformation log(Y + 1) qui préserve les zéros
treedensity_log <- log(treedensity + 1)

cat("Après transformation log(Y+1) :\n")
cat("Valeur minimale :", min(treedensity_log), "\n")
cat("Valeur maximale :", max(treedensity_log), "\n\n")

# Refaire la régression avec Y transformé
data_regression_log <- data.frame(
  treedensity_log = treedensity_log,
  PC1 = composantes_retenues[, 1],
  PC2 = composantes_retenues[, 2],
  PC3 = composantes_retenues[, 3]
)

modele_log <- lm(treedensity_log ~ PC1 + PC2 + PC3, data = data_regression_log)

# Afficher le résumé du modèle log
summary(modele_log)

# Comparer les deux modèles
cat("Modèle original    - R² =", round(summary(modele_complet)$r.squared, 3), "\n")
cat("Modèle log(Y+1)    - R² =", round(summary(modele_log)$r.squared, 3), "\n")
cat("Amélioration du R² :", 
    round(summary(modele_log)$r.squared - summary(modele_complet)$r.squared, 3), "\n\n")

```

```{r}

# Préparer les graphiques côte à côte
par(mfrow = c(1, 2))

# Graphique 1 : Modèle original
Y <- treedensity
Y_chapeau <- fitted(modele_complet)

plot(Y, Y_chapeau, 
     xlab = "Densité observée (Y)", 
     ylab = "Densité prédite (Ŷ)",
     main = paste0("Modèle original\nR² = ", round(summary(modele_complet)$r.squared, 3)),
     pch = 20,
     col = rgb(0, 0, 1, 0.5),
     cex.main = 0.9)

abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_chapeau ~ Y), col = "blue", lty = 2, lwd = 2)

# Graphique 2 : Modèle avec log
Y_log <- treedensity_log
Y_chapeau_log <- fitted(modele_log)

plot(Y_log, Y_chapeau_log, 
     xlab = "log(Densité + 1) observé", 
     ylab = "log(Densité + 1) prédit",
     main = paste0("Modèle log(Y+1)\nR² = ", round(summary(modele_log)$r.squared, 3)),
     pch = 20,
     col = rgb(0, 0.6, 0, 0.5),
     cex.main = 0.9)

abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_chapeau_log ~ Y_log), col = "darkgreen", lty = 2, lwd = 2)

# Réinitialiser les paramètres graphiques
par(mfrow = c(1, 1))

# Calculer les corrélations pour comparer
cor_original <- cor(Y, Y_chapeau)
cor_log <- cor(Y_log, Y_chapeau_log)

cat("\n=== CORRÉLATIONS Y vs Ŷ ===\n")
cat("Modèle original :", round(cor_original, 3), "\n")
cat("Modèle log      :", round(cor_log, 3), "\n")

```

## 3. Seconde régression sur composantes principales

On partitionne les variables explicatives en photosynthèse (EVI) et géographie (tout les reste).

```{r}

# === THÈME GÉOGRAPHIE ===
# Variables géographiques quantitatives + leurs carrés + indicatrices géologie
var_geo <- bind_cols(
  data %>% select(lat, lon, altitude, pluvio_yr, num_range("pluvio_", 1:12)),  # Variables géo
  var_geo_carre,                                                               # Leurs carrés
  as.data.frame(geo_indic)                                                     # Indicatrices geology
)

cat("=== THÈME GÉOGRAPHIE ===\n")
cat("Nombre de variables :", ncol(var_geo), "\n")
cat("Variables :", colnames(var_geo), "\n\n")

# === THÈME PHOTOSYNTHÈSE ===
# Tous les indices EVI
var_photo <- data %>% select(num_range("evi_", 1:23))

cat("=== THÈME PHOTOSYNTHÈSE ===\n")
cat("Nombre de variables :", ncol(var_photo), "\n")
cat("Variables :", colnames(var_photo), "\n\n")

```

*a) Réaliser une ACP réduite globale des variables explicatives. Retenir les composantes qui ne sont pas du bruit.*

```{r}

# ACP réduite sur les variables géographiques
acp_geo <- prcomp(var_geo, scale. = TRUE, center = TRUE)

# Résumé de l'ACP géographie
cat("=== ACP THÈME GÉOGRAPHIE ===\n\n")
summary(acp_geo)

# Variance expliquée par composante
variance_geo <- acp_geo$sdev^2 / sum(acp_geo$sdev^2) * 100

# Graphique de la variance expliquée
barplot(variance_geo[1:15], 
        names.arg = 1:15,
        xlab = "Composante principale", 
        ylab = "% de variance expliquée",
        main = "ACP Géographie - Variance expliquée",
        col = "steelblue")

# Variance cumulée
variance_cum_geo <- cumsum(variance_geo)

# Tableau récapitulatif
recap_geo <- data.frame(
  Composante = 1:length(variance_geo),
  Variance = acp_geo$sdev^2,
  Variance_pct = variance_geo,
  Variance_cum = variance_cum_geo
)

cat("\n")
print(head(recap_geo, 10))

cat("\n=== CHOIX DU NOMBRE DE COMPOSANTES GÉOGRAPHIE ===\n")
cat("Regardez le graphique et le tableau pour décider combien de composantes retenir.\n")
cat("Critères : saut dans la variance expliquée, variance cumulée > 70-80%\n\n")

```

La composante 1 explique beaucoup de variance par rapport à celle d'avant.

2 composantes ??

```{r}

# ACP réduite sur les variables de photosynthèse (EVI)
acp_photo <- prcomp(var_photo, scale. = TRUE, center = TRUE)

# Résumé de l'ACP photosynthèse
cat("=== ACP THÈME PHOTOSYNTHÈSE ===\n\n")
summary(acp_photo)

# Variance expliquée par composante
variance_photo <- acp_photo$sdev^2 / sum(acp_photo$sdev^2) * 100

# Graphique de la variance expliquée
barplot(variance_photo[1:15], 
        names.arg = 1:15,
        xlab = "Composante principale", 
        ylab = "% de variance expliquée",
        main = "ACP Photosynthèse - Variance expliquée",
        col = "darkgreen")

# Variance cumulée
variance_cum_photo <- cumsum(variance_photo)

# Tableau récapitulatif
recap_photo <- data.frame(
  Composante = 1:length(variance_photo),
  Variance = acp_photo$sdev^2,
  Variance_pct = variance_photo,
  Variance_cum = variance_cum_photo
)

cat("\n")
print(head(recap_photo, 10))

cat("\n=== CHOIX DU NOMBRE DE COMPOSANTES PHOTOSYNTHÈSE ===\n")
cat("Regardez le graphique et le tableau pour décider combien de composantes retenir.\n\n")

```

3 composantes principales ??

5 en tout

*b) Modéliser la densité à partir des composantes retenues. Vous pourrez éliminer les composantes qui n'ont pas de rôle statistiquement significatif (justifiez la validité des tests de Student ici). Donner le R2 du modèle obtenu. Construire le graphe d'abscisse Y et d'ordonnée ̂Y , et commenter.*

```{r}

# Définir le nombre de composantes retenues pour chaque thème
n_comp_geo <- 2         # 2 composantes pour Géographie
n_comp_photo <- 3       # 3 composantes pour Photosynthèse

# Récupérer les composantes principales de chaque ACP
comp_geo <- acp_geo$x[, 1:n_comp_geo]
comp_photo <- acp_photo$x[, 1:n_comp_photo]

# Créer un data frame avec Y transformé et TOUTES les composantes
# On réunit les composantes des deux thèmes
data_regression_themes <- data.frame(
  treedensity_log = treedensity_log,  # On utilise la transformation log(Y+1)
  # Composantes Géographie
  PC_geo1 = comp_geo[, 1],
  PC_geo2 = comp_geo[, 2],
  # Composantes Photosynthèse
  PC_photo1 = comp_photo[, 1],
  PC_photo2 = comp_photo[, 2],
  PC_photo3 = comp_photo[, 3]
)

# Ajuster le modèle linéaire avec toutes les composantes
modele_themes <- lm(treedensity_log ~ PC_geo1 + PC_geo2 + 
                                       PC_photo1 + PC_photo2 + PC_photo3, 
                    data = data_regression_themes)

# Afficher le résumé du modèle
summary(modele_themes)

# Comparer avec le modèle de la question 2
cat("Q2 - ACP globale     : R² =", round(summary(modele_log)$r.squared, 3), "\n")
cat("Q3 - ACP par thèmes  : R² =", round(summary(modele_themes)$r.squared, 3), "\n")
cat("Différence           :", 
    round(summary(modele_themes)$r.squared - summary(modele_log)$r.squared, 3), "\n\n")

```

Toutes les cinq composantes (2 de Géographie + 3 de Photosynthèse) sont statistiquement significatives dans le modèle. Les deux composantes géographiques sont très hautement significatives (p-value < 0.001), tout comme deux des trois composantes de photosynthèse (PC_photo2 et PC_photo3). La première composante de photosynthèse (PC_photo1) est significative au seuil de 5% (p-value = 0.030).

Le R² du modèle est de 0.266, ce qui représente une amélioration de 0.0092 points par rapport au modèle de la question 2 (R² = 0.2567). Cette amélioration, bien que modeste, montre que la séparation en thèmes permet de mieux structurer l'information : les variables géographiques et de photosynthèse capturent des aspects complémentaires de la densité arborée.

Les tests de Student sont valides ici pour les mêmes raisons qu'en question 2 : les composantes principales issues de chaque ACP sont orthogonales au sein de leur thème, et les deux thèmes sont analysés séparément avant d'être réunis, ce qui élimine les problèmes de multicolinéarité.

```{r}

# Construire le graphe Y observé vs Y prédit
Y_log <- treedensity_log
Y_chapeau_themes <- fitted(modele_themes)

# Créer le graphique
plot(Y_log, Y_chapeau_themes, 
     xlab = "log(Densité + 1) observé", 
     ylab = "log(Densité + 1) prédit",
     main = paste0("Modèle ACP par thèmes\nR² = ", round(summary(modele_themes)$r.squared, 3)),
     pch = 20,
     col = rgb(0.8, 0.2, 0, 0.5),
     cex.main = 0.9)

# Ajouter la droite identité (y = x)
abline(a = 0, b = 1, col = "red", lwd = 2)

# Ajouter la droite de régression
abline(lm(Y_chapeau_themes ~ Y_log), col = "darkorange", lty = 2, lwd = 2)

legend("topleft", 
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "darkorange"), 
       lty = c(1, 2), 
       lwd = 2,
       cex = 0.8)

# Calculer la corrélation
correlation_themes <- cor(Y_log, Y_chapeau_themes)
cat("\nCorrélation entre Y et Ŷ :", round(correlation_themes, 3), "\n")

```

Le graphique des valeurs observées versus prédites montre une amélioration par rapport au modèle de la question 2 :

- La droite de régression (orange) se rapproche légèrement de la droite identité (rouge), bien qu'un écart persiste encore.
- Les points sont mieux alignés autour de la droite de régression, ce qui indique une meilleure qualité de prédiction.
- La dispersion reste importante, mais elle est plus homogène sur toute la plage de valeurs, ce qui suggère que la transformation logarithmique a bien corrigé l'hétéroscédasticité.
- Le modèle prédit mieux les densités intermédiaires (log(Y+1) entre 2 et 3), mais sous-estime encore légèrement les très fortes densités.

L'amélioration du R² (+1 point) se traduit visuellement par un nuage de points légèrement plus resserré autour de la droite de régression. La séparation en thèmes Géographie/Photosynthèse apporte donc un gain modeste mais réel.

*c) Retrouver les coefficients des variables originelles dans le prédicteur linéaire Y chapeau.*

```{r}

# Récupérer les coefficients des composantes dans le modèle (sans l'intercept)
coef_modele_themes <- coef(modele_themes)[-1]  
print(coef_modele_themes)

# Matrices de rotation (loadings) pour chaque thème
rotation_geo <- acp_geo$rotation[, 1:n_comp_geo]
rotation_photo <- acp_photo$rotation[, 1:n_comp_photo]

# Calculer les coefficients pour les variables GÉOGRAPHIE originales (Formule : β_originaux = Rotation × β_composantes)
coef_geo_originaux <- rotation_geo %*% coef_modele_themes[1:n_comp_geo]

# Calculer les coefficients pour les variables PHOTOSYNTHÈSE originales
coef_photo_originaux <- rotation_photo %*% coef_modele_themes[(n_comp_geo+1):(n_comp_geo+n_comp_photo)]

# Créer des data frames pour une meilleure visualisation
coef_geo_df <- data.frame(
  Variable = rownames(coef_geo_originaux),
  Coefficient = as.vector(coef_geo_originaux)
)

coef_photo_df <- data.frame(
  Variable = rownames(coef_photo_originaux),
  Coefficient = as.vector(coef_photo_originaux)
)

# Trier par valeur absolue décroissante
coef_geo_df <- coef_geo_df[order(abs(coef_geo_df$Coefficient), decreasing = TRUE), ]
coef_photo_df <- coef_photo_df[order(abs(coef_photo_df$Coefficient), decreasing = TRUE), ]

print(head(coef_geo_df, 10), row.names = FALSE)
print(head(coef_photo_df, 10), row.names = FALSE)

# Combiner tous les coefficients pour la synthèse finale (Question 6)
coef_themes_tous <- rbind(
  data.frame(Variable = coef_geo_df$Variable, Coefficient = coef_geo_df$Coefficient, Theme = "Géographie"),
  data.frame(Variable = coef_photo_df$Variable, Coefficient = coef_photo_df$Coefficient, Theme = "Photosynthèse")
)

coef_themes_tous_sorted <- coef_themes_tous[order(abs(coef_themes_tous$Coefficient), decreasing = TRUE), ]
print(coef_themes_tous_sorted)

```

L'analyse des coefficients des variables originelles révèle des patterns intéressants :

**Variables géographiques les plus influentes :**
- **Longitude et son carré (lon, lon^2)** : coefficients positifs importants (~0.019), confirmant que la densité augmente vers l'est du bassin du Congo. La présence du terme quadratique suggère une relation non-linéaire.
- **Pluviométries mensuelles** : juillet (pluvio_7, +0.0165) a un effet positif, tandis qu'octobre et novembre (pluvio_10, pluvio_11, ~-0.016) ont des effets négatifs. Cela indique que les pluies d'été favorisent la densité, mais qu'un excès de pluie en fin d'année peut être défavorable.
- **Termes quadratiques des pluviométries** : leur présence confirme des relations non-linéaires entre climat et densité arborée.

**Variables de photosynthèse les plus influentes :**
- **EVI de fin d'année (evi_19 à evi_23)** : coefficients négatifs forts (~-0.022 à -0.025), suggérant qu'une forte activité photosynthétique en fin d'année est associée à de plus faibles densités. Cela pourrait refléter une concurrence accrue ou des conditions écologiques particulières.
- **EVI de mi-année (evi_11 à evi_13)** : coefficients positifs (~0.016 à 0.020), indiquant qu'une bonne activité photosynthétique en milieu d'année favorise la densité.

**Synthèse globale (TOP 15) :**
Les variables de photosynthèse (EVI) dominent le classement en valeur absolue, ce qui suggère que l'activité photosynthétique est un prédicteur plus fort de la densité que les variables géographiques seules. Cependant, les variables géographiques (longitude, pluviométries) restent importantes et apportent une information complémentaire, ce qui justifie l'approche par thèmes.

*d) Si besoin est, corriger la linéarité de la liaison en utilisant une transformation de type Log sur Y qui laisse les zéros invariants.*

Comme pour la question 2, nous avons utilisé la transformation log(Y+1) sur la variable dépendante dès le départ de la question 3.

Raisons de cette transformation :
1. Correction de la linéarité : améliore la relation linéaire entre Y et les prédicteurs
2. Stabilisation de la variance : réduit l'hétéroscédasticité des résidus
3. Préservation des zéros : log(0+1) = 0, donc les parcelles sans arbres restent à 0

```{r}

cat("Résultats avec transformation log(Y+1) :\n")
cat("- R² = ", round(summary(modele_themes)$r.squared, 3), "\n")
cat("- Erreur standard résiduelle = ", round(summary(modele_themes)$sigma, 3), "\n\n")

```
**Synthèse de la question 3 :**

L'approche par ACP séparées sur les thèmes Géographie et Photosynthèse apporte une amélioration modeste mais significative par rapport à l'ACP globale :
- Le R² passe de 0.2644 à 0.2741 (+1 point)
- Les 6 composantes (3 par thème) sont toutes statistiquement significatives
- La séparation thématique permet de mieux structurer l'information et de distinguer les contributions respectives de la géographie et de la photosynthèse
- Les variables de photosynthèse (EVI) apparaissent comme les prédicteurs les plus forts en valeur absolue
- La transformation log(Y+1) reste nécessaire pour assurer la linéarité et l'homoscédasticité

Cette méthode constitue un compromis intéressant entre l'ACP globale (qui mélange tous les types de variables) et une approche totalement supervisée (comme la régression PLS que nous verrons en question 4).

## Question 4. Régression PLS

*a) Utiliser la régression PLS pour modéliser au mieux la densité (ou sa transformation). Vous utiliserez la validation croisée (de type Leave K out ou K-fold) pour déterminer le meilleur nombre de composantes.*

```{r}

# Préparer les données : X = var_explicatives, Y = treedensity_log
X <- as.matrix(var_explicatives)
Y <- treedensity_log

# Régression PLS avec validation croisée (10-fold cross-validation)
# On teste jusqu'à 20 composantes maximum
set.seed(123)  # Pour la reproductibilité
pls_model <- plsr(Y ~ X, 
                  ncomp = 20,              # Nombre max de composantes à tester
                  scale = TRUE,             # Centrer et réduire les variables
                  validation = "CV",        # Validation croisée
                  segments = 10)            # 10-fold CV

# Résumé du modèle
cat("\n=== RÉSUMÉ DU MODÈLE PLS ===\n")
summary(pls_model)

# Afficher les erreurs de prédiction (PRESS) pour chaque nombre de composantes
cat("\n=== ERREUR DE VALIDATION CROISÉE (PRESS) ===\n")
press_values <- pls_model$validation$PRESS[1, ]  # Extraire correctement les valeurs
cat("Longueur de PRESS :", length(press_values), "\n")
print(data.frame(
  Composantes = 1:length(press_values),
  PRESS = press_values
))

# Graphique des erreurs de validation croisée
par(mfrow = c(1, 2))

# Graphique 1 : PRESS en fonction du nombre de composantes
plot(1:length(press_values), press_values, 
     type = "b", 
     xlab = "Nombre de composantes", 
     ylab = "PRESS (erreur de prédiction)",
     main = "Validation croisée PLS",
     col = "blue",
     pch = 19)
grid()

# Graphique 2 : R² en fonction du nombre de composantes
r2_values <- R2(pls_model)$val[1, 1, ]  # R² de validation croisée
plot(1:length(r2_values), r2_values,
     type = "b",
     xlab = "Nombre de composantes",
     ylab = "R² (validation croisée)",
     main = "R² en fonction du nombre de composantes",
     col = "darkgreen",
     pch = 19)
grid()

par(mfrow = c(1, 1))

# Trouver le nombre optimal de composantes (minimum de PRESS)
optimal_ncomp <- which.min(press_values)
cat("\n=== NOMBRE OPTIMAL DE COMPOSANTES ===\n")
cat("Nombre de composantes minimisant PRESS :", optimal_ncomp, "\n")
cat("PRESS minimal :", min(press_values), "\n\n")

# Ajuster le modèle final avec le nombre optimal de composantes
pls_final <- plsr(Y ~ X, 
                  ncomp = optimal_ncomp,
                  scale = TRUE)

# Résumé du modèle final
cat("\n=== MODÈLE PLS FINAL ===\n")
cat("Nombre de composantes retenues :", optimal_ncomp, "\n")
summary(pls_final)

# Calculer le R² du modèle final
Y_pred_pls <- predict(pls_final, ncomp = optimal_ncomp)
r2_pls <- cor(Y, Y_pred_pls)^2
cat("\nR² du modèle PLS final :", round(r2_pls, 3), "\n")

# Graphique Y observé vs Y prédit
plot(Y, Y_pred_pls,
     xlab = "log(Densité + 1) observé",
     ylab = "log(Densité + 1) prédit",
     main = paste0("Modèle PLS (", optimal_ncomp, " composantes)\nR² = ", round(r2_pls, 3)),
     pch = 20,
     col = rgb(0.6, 0, 0.6, 0.5))

abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_pred_pls ~ Y), col = "purple", lty = 2, lwd = 2)

legend("topleft",
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "purple"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.8)

```

La validation croisée (10-fold) suggère un nombre optimal de 20 composantes pour minimiser le critère PRESS. Cependant, ce choix nous semble excessif et présente un risque évident de sur-ajustement : utiliser 20 composantes sur 60 variables explicatives représente un tiers des variables, ce qui va à l'encontre du principe de parcimonie. De plus, nous pouvons voir que, pour le press, il y a un minimum local atteit, puis ça remonte et ensuite ça resdecend donc surajustement visiblE. De meêm pour le r^2 qui augmnter pour se stabilier et re augmenter par la suite.

En observant lesvaleurs obtenues, nous constatons que :

Le R² augmente rapidement jusqu'à environ 6 composantes, où il atteint approximativement 0.36, puis se stabilise progressivement. L'augmentation devient très marginale au-delà de ce seuil.
Le PRESS diminue fortement jusqu'à 6 composantes (environ 154.5), puis continue à décroître très lentement. La différence entre 6 et 20 composantes est faible au regard de la complexité ajoutée.
Au-delà de 6 composantes, le gain en termes de qualité de prédiction ne justifie plus l'augmentation de la complexité du modèle.

Ainsi, nous retenons 6 composantes pour notre modèle PLS final.
Cette décision repose sur un compromis entre qualité d'ajustement et parcimonie : 6 composantes permettent de capturer l'essentiel de l'information pertinente pour prédire la densité, tout en maintenant un modèle suffisamment simple et interprétable.

*b) Tenter au mieux d'interpréter les composantes retenues, et les plans qu'elles engendrent, exactement comme pour une ACP réduite.*

```{r}

# Redéfinir le nombre de composantes retenues
optimal_ncomp_choisi <- 5

# Les loadings : contribution des variables originelles aux composantes PLS
loadings_pls <- loadings(pls_final)

cat("Dimensions des loadings :", dim(loadings_pls), "\n\n")

# Pour chaque composante, identifier les variables les plus importantes
for (comp in 1:optimal_ncomp_choisi) {
  cat("=== COMPOSANTE PLS", comp, "===\n\n")
  
  # Extraire les loadings de cette composante
  load_comp <- loadings_pls[, comp]
  
  # Créer un data frame avec noms et valeurs
  load_df <- data.frame(
    Variable = names(load_comp),
    Loading = as.numeric(load_comp)
  )
  
  # Trier par valeur absolue décroissante
  load_df <- load_df[order(abs(load_df$Loading), decreasing = TRUE), ]
  
  cat("Top 10 des variables les plus importantes (en valeur absolue) :\n")
  print(head(load_df, 10), row.names = FALSE)
  cat("\n")
}

# === CERCLES DE CORRÉLATION (VARIABLES) ===
cat("\n=== CERCLES DE CORRÉLATION DES VARIABLES ===\n\n")

# Récupérer les scores (coordonnées des individus)
scores_pls <- scores(pls_final)

# Calculer les corrélations entre variables originelles et composantes PLS
correlations <- cor(X, scores_pls)

# Pourcentages de variance expliquée
var_exp <- pls_final$Xvar / sum(pls_final$Xvar) * 100

# === PLAN 1-2 : VARIABLES AVEC FLÈCHES ===
par(mfrow = c(1, 2))

plot(0, 0, 
     xlim = c(-1, 1), ylim = c(-1, 1),
     xlab = paste0("Comp 1 (", round(var_exp[1], 1), "%)"),
     ylab = paste0("Comp 2 (", round(var_exp[2], 1), "%)"),
     main = "Cercle des corrélations (Plan 1-2)",
     type = "n",
     asp = 1)

# Ajouter le cercle
symbols(0, 0, circles = 1, inches = FALSE, add = TRUE, fg = "red", lwd = 2)
abline(h = 0, v = 0, lty = 2, col = "gray50")

# Identifier les variables bien représentées
contrib_12 <- correlations[, 1]^2 + correlations[, 2]^2
seuil <- 0.3  # Seuil de contribution pour afficher

# Dessiner les flèches pour toutes les variables
for (i in 1:nrow(correlations)) {
  if (contrib_12[i] > seuil) {
    arrows(0, 0, 
           correlations[i, 1], correlations[i, 2],
           length = 0.1, 
           col = ifelse(contrib_12[i] > 0.5, "blue", "gray60"),
           lwd = ifelse(contrib_12[i] > 0.5, 2, 1))
    
    # Ajouter le nom de la variable si bien représentée
    if (contrib_12[i] > 0.5) {
      text(correlations[i, 1] * 1.15, correlations[i, 2] * 1.15,
           labels = rownames(correlations)[i],
           cex = 0.6, col = "darkblue")
    }
  }
}

# === PLAN 1-3 : VARIABLES AVEC FLÈCHES ===
plot(0, 0, 
     xlim = c(-1, 1), ylim = c(-1, 1),
     xlab = paste0("Comp 1 (", round(var_exp[1], 1), "%)"),
     ylab = paste0("Comp 3 (", round(var_exp[3], 1), "%)"),
     main = "Cercle des corrélations (Plan 1-3)",
     type = "n",
     asp = 1)

symbols(0, 0, circles = 1, inches = FALSE, add = TRUE, fg = "red", lwd = 2)
abline(h = 0, v = 0, lty = 2, col = "gray50")

contrib_13 <- correlations[, 1]^2 + correlations[, 3]^2

for (i in 1:nrow(correlations)) {
  if (contrib_13[i] > seuil) {
    arrows(0, 0, 
           correlations[i, 1], correlations[i, 3],
           length = 0.1, 
           col = ifelse(contrib_13[i] > 0.5, "darkgreen", "gray60"),
           lwd = ifelse(contrib_13[i] > 0.5, 2, 1))
    
    if (contrib_13[i] > 0.5) {
      text(correlations[i, 1] * 1.15, correlations[i, 3] * 1.15,
           labels = rownames(correlations)[i],
           cex = 0.6, col = "darkgreen")
    }
  }
}

par(mfrow = c(1, 1))

# === NUAGE DES INDIVIDUS ===
cat("\n=== NUAGE DES INDIVIDUS ===\n\n")

par(mfrow = c(1, 2))

# Plan 1-2 : INDIVIDUS colorés selon Y
plot(scores_pls[, 1], scores_pls[, 2],
     xlab = paste0("Comp 1 (", round(var_exp[1], 1), "%)"),
     ylab = paste0("Comp 2 (", round(var_exp[2], 1), "%)"),
     main = "Nuage des individus (Plan 1-2)",
     pch = 20,
     col = rgb(Y/max(Y), 0, 1-Y/max(Y), 0.6),
     cex = 0.8)
abline(h = 0, v = 0, lty = 2, col = "gray50")

# Plan 1-3 : INDIVIDUS
plot(scores_pls[, 1], scores_pls[, 3],
     xlab = paste0("Comp 1 (", round(var_exp[1], 1), "%)"),
     ylab = paste0("Comp 3 (", round(var_exp[3], 1), "%)"),
     main = "Nuage des individus (Plan 1-3)",
     pch = 20,
     col = rgb(Y/max(Y), 0, 1-Y/max(Y), 0.6),
     cex = 0.8)
abline(h = 0, v = 0, lty = 2, col = "gray50")

par(mfrow = c(1, 1))

cat("Note : Les individus sont colorés selon leur densité observée\n")
cat("(Rouge = densité élevée, Bleu = densité faible)\n\n")

```


*c) Retrouver les coefficients des variables (et interactions) originelles dans le prédicteur linéaire ̂ Y*

```{r}

# Méthode 2 : Via l'objet coefficients directement
  if ("coefficients" %in% names(pls_final)) {
    coef_matrix <- pls_final$coefficients
    cat("Dimensions de coefficients :", dim(coef_matrix), "\n")
    
    if (length(dim(coef_matrix)) == 3) {
      coef_pls_vector <- as.vector(coef_matrix[, 1, optimal_ncomp_pls])
    } else {
      coef_pls_vector <- as.vector(coef_matrix[, optimal_ncomp_pls])
    }
    cat("Nouvelle longueur :", length(coef_pls_vector), "\n")
  }
}

# Créer le data frame
if (length(coef_pls_vector) == ncol(X_matrix)) {
  coef_pls_df <- data.frame(
    Variable = colnames(X_matrix),
    Coefficient_PLS = coef_pls_vector,
    stringsAsFactors = FALSE
  )
  
  # Trier par valeur absolue
  coef_pls_df_sorted <- coef_pls_df[order(abs(coef_pls_df$Coefficient_PLS), decreasing = TRUE), ]

  print(coef_pls_df_sorted)
  
} else {
}

```

Les coefficients PLS sont directement les coefficients de régression des variables originelles, contrairement à l'ACP où il faut les reconstituer. C'est une des forces de la PLS : elle donne directement l'effet de chaque variable sur Y.

*d) Comparer l'ajustement et les coefficients du modèle obtenu avec ceux des régressions sur composantes principales (on prêtera notamment attention aux signes de ces coefficients).*


## Question 5. Régressions pénalisées %%%%%%%%%%%%%%%%%%%%%%

*a) Utiliser la régression ridge (package glmnet) pour modéliser au mieux la densité (ou sa transformation). Vous déterminerez le poids optimal de la pénalité par validation croisée.*


### 5.a) Régression Ridge

```{r}

# Préparer les données
X_matrix <- as.matrix(var_explicatives)
Y_vector <- treedensity_log

# Régression Ridge avec validation croisée
set.seed(123)
cv_ridge <- cv.glmnet(X_matrix, Y_vector,
                      alpha = 0,
                      nfolds = 10,
                      standardize = TRUE)

# Lambda optimal
lambda_optimal_ridge <- cv_ridge$lambda.min
lambda_1se_ridge <- cv_ridge$lambda.1se

cat("Lambda min :", lambda_optimal_ridge, "\n")
cat("Lambda 1se :", lambda_1se_ridge, "\n")

# Graphiques de validation croisée

plot(cv_ridge, col = "black", main = "Validation croisée Ridge")
abline(v = log(lambda_optimal_ridge), col = "red", lty = 2, lwd = 2)
abline(v = log(lambda_1se_ridge), col = "blue", lty = 2, lwd = 2)
legend("topright", 
       legend = c("lambda.min", "lambda.1se"),
       col = c("red", "blue"),
       lty = 2,
       cex = 0.8)

# Modèle final avec lambda.1se
ridge_final <- glmnet(X_matrix, Y_vector, 
                      alpha = 0, 
                      lambda = lambda_1se_ridge,
                      standardize = TRUE)

# Coefficients
coef_ridge <- coef(ridge_final)

# Prédictions et R²
Y_pred_ridge <- predict(ridge_final, newx = X_matrix, s = lambda_1se_ridge)
r2_ridge <- cor(Y_vector, Y_pred_ridge)^2
mse_ridge <- mean((Y_vector - Y_pred_ridge)^2)

cat("\nR² du modèle Ridge :", round(r2_ridge, 3), "\n")
cat("MSE du modèle Ridge :", round(mse_ridge, 3), "\n")

# Graphique Y vs Ŷ
plot(Y_vector, Y_pred_ridge,
     xlab = "log(Y + 1) observé",
     ylab = "log(Y + 1) prédit",
     main = paste0("Modèle Ridge\nR² = ", round(r2_ridge, 3)),
     pch = 20,
     col = rgb(0.8, 0.4, 0, 0.5))

abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_pred_ridge ~ Y_vector), col = "darkorange", lty = 2, lwd = 2)

legend("topleft",
       legend = c("Droite identité (y = x)", "Droite de régression"),
       col = c("red", "darkorange"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.8)

# Top 15 des coefficients
coef_ridge_df <- data.frame(
  Variable = rownames(coef_ridge)[-1],
  Coefficient = as.vector(coef_ridge)[-1]
)
coef_ridge_df <- coef_ridge_df[order(abs(coef_ridge_df$Coefficient), decreasing = TRUE), ]

print(coef_ridge_df)

```

*b) Utiliser la régression LASSO (package glmnet) pour modéliser au mieux la densité (ou sa transformation). Vous déterminerez le poids optimal de la pénalité par validation croisée.*

```{r}

# Régression LASSO avec validation croisée
set.seed(123)
cv_lasso <- cv.glmnet(X_matrix, Y_vector,
                      alpha = 1,        # LASSO
                      nfolds = 10,
                      standardize = TRUE)

# Lambda optimal
lambda_optimal_lasso <- cv_lasso$lambda.min
lambda_1se_lasso <- cv_lasso$lambda.1se

cat("Lambda min :", lambda_optimal_lasso, "\n")
cat("Lambda 1se :", lambda_1se_lasso, "\n")

# Graphiques de validation croisée

plot(cv_lasso, main = "Validation croisée LASSO")
abline(v = log(lambda_optimal_lasso), col = "red", lty = 2)
abline(v = log(lambda_1se_lasso), col = "blue", lty = 2)
legend("top", 
       legend = c("lambda.min", "lambda.1se"),
       col = c("red", "blue"),
       lty = 2,
       cex = 0.8)

# Modèle final avec lambda.1se
lasso_final <- glmnet(X_matrix, Y_vector, 
                      alpha = 1, 
                      lambda = lambda_1se_lasso,
                      standardize = TRUE)

# Coefficients
coef_lasso <- coef(lasso_final)
nb_nonzero <- sum(coef_lasso[-1] != 0)

cat("\nNombre de variables sélectionnées (coef ≠ 0) :", nb_nonzero, "sur", ncol(X_matrix), "\n")
cat("Nombre de variables éliminées (coef = 0)     :", ncol(X_matrix) - nb_nonzero, "\n")

# Prédictions et R²
Y_pred_lasso <- predict(lasso_final, newx = X_matrix, s = lambda_1se_lasso)
r2_lasso <- cor(Y_vector, Y_pred_lasso)^2
mse_lasso <- mean((Y_vector - Y_pred_lasso)^2)

cat("\nR² du modèle LASSO  :", round(r2_lasso, 3), "\n")
cat("MSE du modèle LASSO :", round(mse_lasso, 3), "\n")

# Graphique Y vs Ŷ
plot(Y_vector, Y_pred_lasso,
     xlab = "log(Densité + 1) observé",
     ylab = "log(Densité + 1) prédit",
     main = paste0("Modèle LASSO (", nb_nonzero, " variables)\nR² = ", round(r2_lasso, 3)),
     pch = 20,
     col = rgb(0, 0.6, 0.4, 0.5))

abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_pred_lasso ~ Y_vector), col = "darkgreen", lty = 2, lwd = 2)

legend("topleft",
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "darkgreen"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.8)

# Variables sélectionnées (coefficients non nuls)
coef_lasso_df <- data.frame(
  Variable = rownames(coef_lasso)[-1],
  Coefficient = as.vector(coef_lasso)[-1]
)
coef_lasso_nonzero <- coef_lasso_df[coef_lasso_df$Coefficient != 0, ]
coef_lasso_nonzero <- coef_lasso_nonzero[order(abs(coef_lasso_nonzero$Coefficient), decreasing = TRUE), ]

print(coef_lasso_nonzero, row.names = FALSE)

```


```{r}

# Paramètres communs pour tous les graphiques
largeur <- 800
hauteur <- 600
resolution <- 120

# Couleurs pour chaque méthode
couleur_rcp_globale <- rgb(0, 0, 1, 0.5)          # Bleu
couleur_rcp_themes <- rgb(0.8, 0.2, 0, 0.5)       # Orange
couleur_pls <- rgb(0.6, 0, 0.6, 0.5)              # Violet
couleur_ridge <- rgb(0.8, 0.4, 0, 0.5)            # Orange foncé
couleur_lasso <- rgb(0, 0.6, 0.4, 0.5)            # Vert-bleu

# 1. RCP globale (avec transformation log)
png("regression_acp.png", width = largeur, height = hauteur, res = resolution)
plot(treedensity_log, fitted(modele_log), 
     xlab = "log(Densité + 1) observé", 
     ylab = "log(Densité + 1) prédit",
     main = paste0("RCP globale (3 comp.)\nR² = ", round(summary(modele_log)$r.squared, 3)),
     pch = 20, 
     col = couleur_rcp_globale,
     cex.main = 1.2,
     cex.lab = 1.1)
abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(fitted(modele_log) ~ treedensity_log), col = "blue", lty = 2, lwd = 2)
legend("topleft",
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "blue"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.9)
dev.off()

# 2. RCP thèmes
png("graph_RCP_themes.png", width = largeur, height = hauteur, res = resolution)
plot(treedensity_log, fitted(modele_themes), 
     xlab = "log(Densité + 1) observé", 
     ylab = "log(Densité + 1) prédit",
     main = paste0("RCP thèmes (6 comp.)\nR² = ", round(summary(modele_themes)$r.squared, 3)),
     pch = 20, 
     col = couleur_rcp_themes,
     cex.main = 1.2,
     cex.lab = 1.1)
abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(fitted(modele_themes) ~ treedensity_log), col = "darkorange", lty = 2, lwd = 2)
legend("topleft",
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "darkorange"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.9)
dev.off()

# 3. PLS
png("graph_PLS.png", width = largeur, height = hauteur, res = resolution)
plot(Y, Y_pred_pls,
     xlab = "log(Densité + 1) observé",
     ylab = "log(Densité + 1) prédit",
     main = paste0("PLS (6 comp.)\nR² = ", round(r2_pls, 3)),
     pch = 20, 
     col = couleur_pls,
     cex.main = 1.2,
     cex.lab = 1.1)
abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_pred_pls ~ Y), col = "purple", lty = 2, lwd = 2)
legend("topleft",
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "purple"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.9)
dev.off()

# 4. Ridge
png("graph_Ridge.png", width = largeur, height = hauteur, res = resolution)
plot(Y_vector, Y_pred_ridge,
     xlab = "log(Densité + 1) observé",
     ylab = "log(Densité + 1) prédit",
     main = paste0("Ridge (λ = ", round(lambda_1se_ridge, 2), ")\nR² = ", round(r2_ridge, 3)),
     pch = 20, 
     col = couleur_ridge,
     cex.main = 1.2,
     cex.lab = 1.1)
abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_pred_ridge ~ Y_vector), col = "darkorange", lty = 2, lwd = 2)
legend("topleft",
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "darkorange"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.9)
dev.off()

# 5. LASSO
png("graph_LASSO.png", width = largeur, height = hauteur, res = resolution)
plot(Y_vector, Y_pred_lasso,
     xlab = "log(Densité + 1) observé",
     ylab = "log(Densité + 1) prédit",
     main = paste0("LASSO (", nb_nonzero, " var.)\nR² = ", round(r2_lasso, 3)),
     pch = 20, 
     col = couleur_lasso,
     cex.main = 1.2,
     cex.lab = 1.1)
abline(a = 0, b = 1, col = "red", lwd = 2)
abline(lm(Y_pred_lasso ~ Y_vector), col = "darkgreen", lty = 2, lwd = 2)
legend("topleft",
       legend = c("Droite identité (y=x)", "Droite de régression"),
       col = c("red", "darkgreen"),
       lty = c(1, 2),
       lwd = 2,
       cex = 0.9)
dev.off()

```





















